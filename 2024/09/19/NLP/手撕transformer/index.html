<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>手撕transformer - Pluto&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Pluto&#039;s Blog"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/b_0b65080deefa163507d3aa4c6a7a5e07.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Pluto&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Transformer是当代自然语言处理领域不可或缺的模型。本文作者最近学习大语言模型，而Transformer就是大语言模型的基础，所以书写本文通过PyTorch实现Transformer."><meta property="og:type" content="article"><meta property="og:title" content="手撕transformer"><meta property="og:url" content="http://example.com/2024/09/19/NLP/%E6%89%8B%E6%92%95transformer/"><meta property="og:site_name" content="Pluto&#039;s Blog"><meta property="og:description" content="Transformer是当代自然语言处理领域不可或缺的模型。本文作者最近学习大语言模型，而Transformer就是大语言模型的基础，所以书写本文通过PyTorch实现Transformer."><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/avatar.jpg"><meta property="article:published_time" content="2024-09-19T13:14:00.000Z"><meta property="article:modified_time" content="2025-05-31T16:43:21.497Z"><meta property="article:author" content="Pluto"><meta property="article:tag" content="NLP"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/avatar.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2024/09/19/NLP/%E6%89%8B%E6%92%95transformer/"},"headline":"Pluto's Blog","image":["https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/avatar.jpg"],"datePublished":"2024-09-19T13:14:00.000Z","dateModified":"2025-05-31T16:43:21.497Z","author":{"@type":"Person","name":"Pluto"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject"}},"description":"Transformer是当代自然语言处理领域不可或缺的模型。本文作者最近学习大语言模型，而Transformer就是大语言模型的基础，所以书写本文通过PyTorch实现Transformer."}</script><link rel="canonical" href="http://example.com/2024/09/19/NLP/%E6%89%8B%E6%92%95transformer/"><link rel="icon" href="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/b_0b65080deefa163507d3aa4c6a7a5e07.jpg"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/6.0.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.loli.net/ajax/libs/pace/1.2.4/pace.min.js"></script><script async="" referrerpolicy="no-referrer" src="//cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script src="/js/md5.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/b_0b65080deefa163507d3aa4c6a7a5e07.jpg" alt="Pluto&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/1-pluto1"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><!--!--><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/avatar.jpg" alt="手撕transformer"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2024-09-19  <a class="commentCountImg" href="/2024/09/19/NLP/%E6%89%8B%E6%92%95transformer/#comment-container"><span class="display-none-class">/2024/09/19/NLP/手撕transformer/</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="8ad924c6786d313b2e6eb137f9e0659f">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>19 分钟  <i class="fas fa-pencil-alt"> </i>2.8 k</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">手撕transformer</h1><div class="content"><p>Transformer是当代自然语言处理领域不可或缺的模型。本文作者最近学习大语言模型，而Transformer就是大语言模型的基础，所以书写本文通过PyTorch实现Transformer.</p>
<span id="more"></span>
<h3 id="Pytorch实现Transformer"><a href="#Pytorch实现Transformer" class="headerlink" title="Pytorch实现Transformer"></a>Pytorch实现Transformer</h3><p>Transformer是当代自然语言处理领域不可或缺的模型。本文作者最近学习大语言模型，而Transformer就是大语言模型的基础，所以书写本文通过PyTorch实现Transformer.</p>
<h4 id="Preparing"><a href="#Preparing" class="headerlink" title="Preparing"></a>Preparing</h4><p>首先，先导入所需要的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<p>后面需要使用一些有关Transformer的超参数，所以在开头先定义出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d_model = <span class="number">512</span></span><br><span class="line">max_len = <span class="number">1024</span></span><br><span class="line">d_ff = <span class="number">2048</span></span><br><span class="line">d_k = d_v = <span class="number">64</span></span><br><span class="line">n_layers = <span class="number">6</span></span><br><span class="line">n_heads = <span class="number">8</span></span><br><span class="line">p_drop = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>

<p>变量名依次是：</p>
<ul>
<li>d_model: Embedding的大小</li>
<li>max_len: 输入序列的最长大小</li>
<li>d_ff: 前馈神经网络的隐藏层大小，一般是 d_model的四倍</li>
<li>d_k, d_v : 自注意力中K和V的维度， Q的维度直接用K的维度代替，因为这两者必须始终相等</li>
<li>n_layers: Encoder和Decoder的层数</li>
<li>n_heads: 自注意力多头的头数</li>
<li>p_drop: Dropout的概率</li>
</ul>
<h4 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h4><p>Mask分为两种，一种是因为在数据中使用了padding，不希望pad被加入到注意力中进行计算的Pad Mask for Attention，还有一种是保证Decoder自回归信息不泄露的Subsequent Mask for Decoder。</p>
<h5 id="Pad-Mask-for-Attention"><a href="#Pad-Mask-for-Attention" class="headerlink" title="Pad Mask for Attention"></a>Pad Mask for Attention</h5><p>为了方便，假设 <PAD> 在字典中的index是0，遇到输入为0直接将其标为True。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Padding, because of unequal in source_len and target_len.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  parameters:</span></span><br><span class="line"><span class="string">  seq_q: [batch, seq_len]</span></span><br><span class="line"><span class="string">  seq_k: [batch, seq_len]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  return:</span></span><br><span class="line"><span class="string">  mask: [batch, len_q, len_k]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">	batch, len_q = seq_q.size()</span><br><span class="line">    batch, len_k = seq_k.size()</span><br><span class="line">    </span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch, len_q, len_k)</span><br></pre></td></tr></table></figure>

<p>在Encoder和Decoder中使用Mask的情况可能各有不同:</p>
<ul>
<li>在Encoder中使用Mask, 是为了将<code>encoder_input</code>中没有内容而打上PAD的部分进行Mask, 方便矩阵运算.</li>
<li>在Decoder中使用Mask, 可能是在Decoder的自注意力对<code>decoder_input</code> 的PAD进行Mask, 也有可能是对Encoder - Decoder自注意力时对<code>encoder_input</code>和<code>decoder_input</code>的PAD进行Mask</li>
</ul>
<h5 id="Subsequent-Mask-for-Decoder"><a href="#Subsequent-Mask-for-Decoder" class="headerlink" title="Subsequent Mask for Decoder"></a>Subsequent Mask for Decoder</h5><p>该Mask是为了防止Decoder的自回归信息泄露而生的Mask, 直接生成一个上三角矩阵即可:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequent_mask</span>(<span class="params">seq</span>):</span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Build attention mask matrix for decoder when it autoregressing.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  parameters:</span></span><br><span class="line"><span class="string">  seq: [batch, target_len]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  return:</span></span><br><span class="line"><span class="string">  subsequent_mask: [batch, target_len, target_len] </span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)] <span class="comment"># [batch, target_len, target_len]</span></span><br><span class="line">  subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>) <span class="comment"># [batch, target_len, target_len] </span></span><br><span class="line">  subsequent_mask = torch.from_numpy(subsequent_mask)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> subsequent_mask <span class="comment"># [batch, target_len, target_len] </span></span><br></pre></td></tr></table></figure>

<h5 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h5><p>在Transformer中, 使用的是绝对位置编码, 用于传输给模型Self - Attention所不能传输的位置信息, 编码使用正余弦公式实现:</p>
<p><img src="C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20240919133817012.png" alt="image-20240919133817012"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, drop_out=<span class="number">0.1</span>, max_len=<span class="number">1024</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p = p_drop)</span><br><span class="line">        positional_encoding = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-torch.log(torch.Tensor([<span class="number">10000</span>])) / d_model))</span><br><span class="line">        </span><br><span class="line">		positional_encoding[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">         positional_encoding[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">         </span><br><span class="line">         positional_encoding = positional_encoding.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">          <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, postional_encoding)</span><br><span class="line">        </span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:x.size(<span class="number">0</span>), ...]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<p><img src="C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20240919171506817.png" alt="image-20240919171506817"></p>
<h5 id="Feed-Forward-Neural-Network"><a href="#Feed-Forward-Neural-Network" class="headerlink" title="Feed Forward Neural Network"></a>Feed Forward Neural Network</h5><p>在Transformer中, Encoder或者Decoder每个Block都需要用一个前馈神经网络来添加<strong>非线性</strong>:</p>
<p><img src="C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20240919171536276.png" alt="image-20240919171536276"></p>
<p>注意, 这里它们都是有偏置的, 而且这两个Linear可以用两个1×1 的卷积来实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardNetwork</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    using nn.Conv1d replace nn.Linear to implements FFN</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeedForwardNetwork, <span class="variable language_">self</span>).__init__()</span><br><span class="line">         <span class="comment"># self.ff1 = nn.Linear(d_model, d_ff)</span></span><br><span class="line">         <span class="comment"># self.ff2 = nn.Linear(d_ff, d_model)</span></span><br><span class="line">        <span class="variable language_">self</span>.ff1 = nn.Conv1d(d_model, d_ff, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.ff2 = nn.Conv1d(d_ff, d_model, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.Relu()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">         <span class="comment"># x: [batch, seq_len, d_model]</span></span><br><span class="line">         residual = x</span><br><span class="line">         x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">         x = <span class="variable language_">self</span>.ff1(x)</span><br><span class="line">         x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">         x = <span class="variable language_">self</span>.ff2(x)</span><br><span class="line">         x = x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">         <span class="keyword">return</span> <span class="variable language_">self</span>.layer_norm(residual + x)</span><br></pre></td></tr></table></figure>

<p>作为一个子层, 不要忘记Transformer中提到的Residual Connection和Layer Norm.</p>
<p>我选择用两个卷积代替Linear. 在<code>nn.Conv1d</code>中, 要求数据的规格为<code>[batch, x, ...]</code>, 我们是要对<code>d_model</code> 上的数据进行卷积, 所以还是需要<code>transpose</code>一下</p>
<h5 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi - Head Attention"></a>Multi - Head Attention</h5><p>先说多头注意力, 因为多头注意力能够决定缩放点积注意力的输入大小. 作为一个子层, 其中的Residual Connection和Layer Norm是必须的.</p>
<p>多头注意力是多个不同的头来获取不同的特征, 类似于多个<strong>卷积核</strong>所达到的效果. 在计算完后通过一个Linear调整大小:</p>
<p><img src="C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20240919172049566.png" alt="image-20240919172049566"></p>
<p>多头注意力在Encoder和Decoder中的使用略有区别, 主要区别在于Mask的不同. 我们前面已经实现了两种Mask函数, 在这里会用到.</p>
<p>多头注意力实际上不是通过弄出很多大小相同的矩阵然后相乘来实现的, 只需要合并到一个矩阵进行计算:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_heads = <span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_heads = n_heads</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.W_Q == nn.Linear(d_model, d_k * n_heads, bias = <span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.W_K == nn.Linear(d_model, d_k * n_heads, bias = <span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.W_V == nn.Linear(d_model, d_v * n_heads, bias = <span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(d_v * n_heads, d_model, bias)</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(d_model)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_Q, input_K, input_V, attn_mask</span>):</span><br><span class="line">        residual, batch = input_Q, input_Q.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        Q = <span class="variable language_">self</span>.W_Q(input_Q).view(batch, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = <span class="variable language_">self</span>.W_K(input_K).view(batch, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    	V = <span class="variable language_">self</span>.W_V(input_V).view(batch, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        prob, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)</span><br><span class="line"></span><br><span class="line">    	prob = prob.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">    	prob = prob.view(batch, -<span class="number">1</span>, n_heads * d_v).contiguous()</span><br><span class="line"></span><br><span class="line">    	output = <span class="variable language_">self</span>.fc(prob) <span class="comment"># [batch, len_q, d_model]</span></span><br><span class="line"></span><br><span class="line">    	<span class="keyword">return</span> <span class="variable language_">self</span>.layer_norm(residual + output), attn</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>提两个非常重要的点:</p>
<ol>
<li>在拆维度时不要破坏维度原来本身的意义.</li>
<li>虽然新版本已经有<code>reshape</code>函数可以用了, 但是仍然不要忘记, <code>transpose</code>后如果接<code>permute</code>或者<code>view</code>必须要加<code>contiguous</code>, 这是<strong>数据真实存储连续与否</strong>的问题, 请参见<a target="_blank" rel="noopener" href="https://adaning.github.io/posts/42255.html">Pytorch之张量基础操作</a>中的<strong>维度变换</strong>部分</li>
</ol>
<h5 id="Scaled-DotProduct-Attention"><a href="#Scaled-DotProduct-Attention" class="headerlink" title="Scaled DotProduct Attention"></a>Scaled DotProduct Attention</h5><p>Tranformer中非常重要的概念, 缩放点积注意力, 公式如下:<br><img src="C:\Users\Pluto\AppData\Roaming\Typora\typora-user-images\image-20240919173831098.png" alt="image-20240919173831098"><br>实现起来非常简单, 只需要把Q, K两个矩阵一乘, 然后再缩放, 过一次Softmax, 再和V乘下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">super</span>(ScaledDotProductAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Q: [batch, n_heads, len_q, d_k]</span></span><br><span class="line"><span class="string">    K: [batch, n_heads, len_k, d_k]</span></span><br><span class="line"><span class="string">    V: [batch, n_heads, len_v, d_v]</span></span><br><span class="line"><span class="string">    attn_mask: [batch, n_heads, seq_len, seq_len]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k) <span class="comment"># [batch, n_heads, len_q, len_k]</span></span><br><span class="line">    scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)</span><br><span class="line"></span><br><span class="line">    attn = nn.Softmax(dim=-<span class="number">1</span>)(scores) <span class="comment"># [batch, n_heads, len_q, len_k]</span></span><br><span class="line">    prob = torch.matmul(attn, V) <span class="comment"># [batch, n_heads, len_q, d_v]</span></span><br><span class="line">    <span class="keyword">return</span> prob, attn</span><br></pre></td></tr></table></figure>

<p><code>masked_fill_</code>能把传进来的Mask为True的地方全都填充上某个值, 这里需要用一个很大的负数来保证ex→0, 使得其在Softmax 中可以被忽略</p>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>先写出Encoder的每个Layer, 由多头注意力和FFN组成:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder_self_attn = MultiHeadAttention()</span><br><span class="line">        <span class="variable language_">self</span>.ffn = FeedForwardNetwork()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input, encoder_pad_mask</span>):</span><br><span class="line">        encoder_output, attn = <span class="variable language_">self</span>.encoder_self_attn(encoder_input, encoder_input, encoder_input, encoder_pad_mask )</span><br><span class="line">        encoder_output = <span class="variable language_">self</span>.ffn(encoder_output)</span><br><span class="line">        <span class="keyword">return</span> encoder_output, attn</span><br></pre></td></tr></table></figure>

<p>对于给定的<code>encoder_input</code>和<code>encoder_pad_pask</code>, Encoder应该能够完成整个Block(Layer)的计算流程. 然后实现整个Encoder:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="variable language_">self</span>.source_embedding = nn.Embedding(source_vocab_size, d_model)</span><br><span class="line">    <span class="variable language_">self</span>.positional_embedding = PositionalEncoding(d_model)</span><br><span class="line">    <span class="variable language_">self</span>.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input</span>):</span><br><span class="line">    <span class="comment"># encoder_input: [batch, source_len]</span></span><br><span class="line">    encoder_output = <span class="variable language_">self</span>.source_embedding(encoder_input) <span class="comment"># [batch, source_len, d_model]</span></span><br><span class="line">    encoder_output = <span class="variable language_">self</span>.positional_embedding(encoder_output.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="comment"># [batch, source_len, d_model]</span></span><br><span class="line"></span><br><span class="line">    encoder_self_attn_mask = get_attn_pad_mask(encoder_input, encoder_input) <span class="comment"># [batch, source_len, source_len]</span></span><br><span class="line">    encoder_self_attns = <span class="built_in">list</span>()</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">      <span class="comment"># encoder_output: [batch, source_len, d_model]</span></span><br><span class="line">      <span class="comment"># encoder_self_attn: [batch, n_heads, source_len, source_len]</span></span><br><span class="line">      encoder_output, encoder_self_attn = layer(encoder_output, encoder_self_attn_mask)</span><br><span class="line">      encoder_self_attns.append(encoder_self_attn)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> encoder_output, encoder_self_attns</span><br></pre></td></tr></table></figure>

<p>对于整个Encoder, 直接将Token的Index传入Embedding中, 再添入位置编码, 之后就经过多层Transformer Encoder. 在传入Block前, 先需要计算Padding的Mask, 再将上层的输出作为下层输入依次迭代.</p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>其实实现了Encoder, Decoder的实现部分都是对应的. 先实现Decoder的Block:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="variable language_">self</span>.decoder_self_attn = MultiHeadAttention()</span><br><span class="line">    <span class="variable language_">self</span>.encoder_decoder_attn = MultiHeadAttention()</span><br><span class="line">    <span class="variable language_">self</span>.ffn = FeedForwardNetwork()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, decoder_input, encoder_output, decoder_self_mask, decoder_encoder_mask</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    decoder_input: [batch, target_len, d_mdoel]</span></span><br><span class="line"><span class="string">    encoder_output: [batch, source_len, d_model]</span></span><br><span class="line"><span class="string">    decoder_self_mask: [batch, target_len, target_len]</span></span><br><span class="line"><span class="string">    decoder_encoder_mask: [batch, target_len, source_len]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># masked mutlihead attention</span></span><br><span class="line">    <span class="comment"># Q, K, V all from decoder it self</span></span><br><span class="line">    <span class="comment"># decoder_output: [batch, target_len, d_model]</span></span><br><span class="line">    <span class="comment"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span></span><br><span class="line">    decoder_output, decoder_self_attn = <span class="variable language_">self</span>.decoder_self_attn(decoder_input, decoder_input, decoder_input, decoder_self_mask)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Q from decoder, K, V from encoder</span></span><br><span class="line">    <span class="comment"># decoder_output: [batch, target_len, d_model]</span></span><br><span class="line">    <span class="comment"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span></span><br><span class="line">    decoder_output, decoder_encoder_attn = <span class="variable language_">self</span>.encoder_decoder_attn(decoder_output, encoder_output, encoder_output, decoder_encoder_mask)</span><br><span class="line">    decoder_output = <span class="variable language_">self</span>.ffn(decoder_output) <span class="comment"># [batch, target_len, d_model]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> decoder_output, decoder_self_attn, decoder_encoder_attn</span><br></pre></td></tr></table></figure>

<p>与Encoder相对应, 只不过因为多了一个Encoder - Decoder自注意力, 所以需要额外计算一个Encoder - Decoder的Mask. 然后写出整个Decoder:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="variable language_">self</span>.target_embedding = nn.Embedding(target_vocab_size, d_model)</span><br><span class="line">    <span class="variable language_">self</span>.positional_embedding = PositionalEncoding(d_model)</span><br><span class="line">    <span class="variable language_">self</span>.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, decoder_input, encoder_input, encoder_output</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    decoder_input: [batch, target_len]</span></span><br><span class="line"><span class="string">    encoder_input: [batch, source_len]</span></span><br><span class="line"><span class="string">    encoder_output: [batch, source_len, d_model]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    decoder_output = <span class="variable language_">self</span>.target_embedding(decoder_input) <span class="comment"># [batch, target_len, d_model]</span></span><br><span class="line">    decoder_output = <span class="variable language_">self</span>.positional_embedding(decoder_output.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="comment"># [batch, target_len, d_model]</span></span><br><span class="line">    decoder_self_attn_mask = get_attn_pad_mask(decoder_input, decoder_input) <span class="comment"># [batch, target_len, target_len]</span></span><br><span class="line">    decoder_subsequent_mask = get_attn_subsequent_mask(decoder_input) <span class="comment"># [batch, target_len, target_len]</span></span><br><span class="line"></span><br><span class="line">    decoder_encoder_attn_mask = get_attn_pad_mask(decoder_input, encoder_input) <span class="comment"># [batch, target_len, source_len]</span></span><br><span class="line"></span><br><span class="line">    decoder_self_mask = torch.gt(decoder_self_attn_mask + decoder_subsequent_mask, <span class="number">0</span>)</span><br><span class="line">    decoder_self_attns, decoder_encoder_attns = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">      <span class="comment"># decoder_output: [batch, target_len, d_model]</span></span><br><span class="line">      <span class="comment"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span></span><br><span class="line">      <span class="comment"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span></span><br><span class="line">      decoder_output, decoder_self_attn, decoder_encoder_attn = layer(decoder_output, encoder_output, decoder_self_mask, decoder_encoder_attn_mask)</span><br><span class="line">      decoder_self_attns.append(decoder_self_attn)</span><br><span class="line">      decoder_encoder_attns.append(decoder_encoder_attn)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> decoder_output, decoder_self_attns, decoder_encoder_attns</span><br></pre></td></tr></table></figure>

<p>和Encoder相对应, 但Decoder和Encoder使用了两个不同的Embedding. 对于Mask, 可以把自回归Mask和Padding Mask用<code>torch.gt</code>整合成一个Mask, 送入其中.</p>
<h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><p>里面有一个Encoder, 一个Decoder, 在Decoder端还需要加上投影层来分类:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = Encoder()</span><br><span class="line">        <span class="variable language_">self</span>.decoder = Decoder()</span><br><span class="line">        sefl.projection = nn.Linear(d_model, target_vovab_size, bias = <span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input, decoder_input</span>):</span><br><span class="line">         <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    encoder_input: [batch, source_len]</span></span><br><span class="line"><span class="string">    decoder_input: [batch, target_len]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># encoder_output: [batch, source_len, d_model]</span></span><br><span class="line">    <span class="comment"># encoder_attns: [n_layers, batch, n_heads, source_len, source_len]</span></span><br><span class="line">    encoder_output, encoder_attns = <span class="variable language_">self</span>.encoder(encoder_input)</span><br><span class="line">    <span class="comment"># decoder_output: [batch, target_len, d_model]</span></span><br><span class="line">    <span class="comment"># decoder_self_attns: [n_layers, batch, n_heads, target_len, target_len]</span></span><br><span class="line">    <span class="comment"># decoder_encoder_attns: [n_layers, batch, n_heads, target_len, source_len]</span></span><br><span class="line">    decoder_output, decoder_self_attns, decoder_encoder_attns = <span class="variable language_">self</span>.decoder(decoder_input, encoder_input, encoder_output)</span><br><span class="line">    decoder_logits = <span class="variable language_">self</span>.projection(decoder_output) <span class="comment"># [batch, target_len, target_vocab_size]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># decoder_logits: [batch * target_len, target_vocab_size]</span></span><br><span class="line">    <span class="keyword">return</span> decoder_logits.view(-<span class="number">1</span>, decoder_logits.size(-<span class="number">1</span>)), encoder_attns, decoder_self_attns, decoder_encoder_attns</span><br></pre></td></tr></table></figure>

<p>最后对logits的处理是<code>view</code>成了<code>[batch * target_len, target_vocab_size]</code>, 前面的大小并不影响我们一会用交叉熵计算损失.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>手撕transformer</p><p><a href="http://example.com/2024/09/19/NLP/手撕transformer/">http://example.com/2024/09/19/NLP/手撕transformer/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><a href="http://example.com"><p>Pluto</p></a></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2024-09-19</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-05-31</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="recommend-area"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/css/share.min.css"><div class="social-share"></div><script src="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/123501B00F7AB6589E40FE43E701F8C9.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/CD85D593847FE4E01CC8386976FEBE69.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2025/05/11/Research/ReasonFlux%EF%BC%9A%20Hierarchical%20LLM%20Reasoning%20via%20Scaling%20Thought%20Templates/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">ReasonFlux阅读</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/07/26/%E7%AE%97%E6%B3%95/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9804/"><span class="level-item">每日一题04</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--><div class="card"><div class="card-content"><div class="title is-5">评论</div><div class="content" id="comment-container"></div><script>var valine = new Valine({
            el: '#comment-container' ,
            notify: false,
            verify: false,
            appId: '21qqhhR0tOxfL1WbwUcK92DR-gzGzoHsz',
            appKey: 'e6qxlPdhLm33xH3N9UZP8Oay',
            placeholder: '留下您的高见！',
            avatar: 'mp',
            avatarForce: false,
            meta: ["nick","mail","link"],
            pageSize: 10,
            visitor: false,
            highlight: true,
            recordIP: false,
            path:'/2024/09/19/NLP/手撕transformer/',
            lang:'en',
            enableQQ:true,
            requiredFields:["nick","mail","link"]
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/b_0b65080deefa163507d3aa4c6a7a5e07.jpg" alt="Pluto"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Pluto</p><p class="is-size-6 is-block">All things come to those who wait.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>四川成都</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">5</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/1-pluto1" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/1-pluto1"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:im.yang.zhao.edu@gmail.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="WeiChat" href="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/443E347C13A787C5F7DB59948B0B130A.png"><i class="fab fa-weixin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="https://qm.qq.com/q/gSHKiKzt6g"><i class="fab fa-qq"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="zhihu" href="https://www.zhihu.com/people/10-65-19-7"><i class="fab fa-zhihu"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"来源《"+data.from+"》</p><p>提供者-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://leetcode.cn/discuss/post/3141566/ru-he-ke-xue-shua-ti-by-endlesscheng-q3yd/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">LeetCode</span></span><span class="level-right"><span class="level-item tag">leetcode.cn</span></span></a></li><li><a class="level is-mobile" href="https://blog.csdn.net/weixin_73184653?spm=1010.2135.3001.10640" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">CSDN</span></span><span class="level-right"><span class="level-item tag">blog.csdn.net</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新评论</h3><span class="body_hot_comment">加载中，最新评论有1分钟缓存...</span></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2025/05/31/Research/Buffer%20of%20Thoughts%EF%BC%9AThought-Augmented%20Reasoning%20with%20Large%20Language%20Models/"><img src="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/20250531220408214.png" alt="Buffer of Thoughts：Thought-Augmented Reasoning with Large Language Models"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-05-31T22:03:07.000Z">2025-05-31</time></p><p class="title"><a href="/2025/05/31/Research/Buffer%20of%20Thoughts%EF%BC%9AThought-Augmented%20Reasoning%20with%20Large%20Language%20Models/">Buffer of Thoughts：Thought-Augmented Reasoning with Large Language Models</a></p><p class="categories"><a href="/categories/Research/">Research</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/05/31/Research/SUPERCORRECT%EF%BC%9ASUPERVISING%20AND%20CORRECTING%20%20LANGUAGE%20MODELS%20WITH%20ERROR-DRIVEN%20INSIGHTS/"><img src="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/20250531212714676.png" alt="SUPERCORRECT：SUPERVISING AND CORRECTING  LANGUAGE MODELS WITH ERROR-DRIVEN INSIGHTS"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-05-31T21:23:55.000Z">2025-05-31</time></p><p class="title"><a href="/2025/05/31/Research/SUPERCORRECT%EF%BC%9ASUPERVISING%20AND%20CORRECTING%20%20LANGUAGE%20MODELS%20WITH%20ERROR-DRIVEN%20INSIGHTS/">SUPERCORRECT：SUPERVISING AND CORRECTING  LANGUAGE MODELS WITH ERROR-DRIVEN INSIGHTS</a></p><p class="categories"><a href="/categories/Research/">Research</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-31T16:43:21.498Z">2025-05-31</time></p><p class="title"><a href="/2025/05/31/%E3%80%90%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B%E3%80%91%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%20Hexo%20%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%9AAmazing%20%E4%B8%BB%E9%A2%98%E3%80%81GitHub%20Pages%20%E9%83%A8%E7%BD%B2%E4%B8%8E%20Github%20actions%20%E8%87%AA%E5%8A%A8%E5%8C%96%EF%BC%8C%E7%BB%93%E5%90%88%20Obsidian%20%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E5%88%9B%E4%BD%9C/"> </a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/05/11/Research/ReasonFlux%EF%BC%9A%20Hierarchical%20LLM%20Reasoning%20via%20Scaling%20Thought%20Templates/"><img src="https://github.com/Gen-Verse/ReasonFlux/raw/main/figs/image.png" alt="ReasonFlux阅读"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-05-11T04:35:40.000Z">2025-05-11</time></p><p class="title"><a href="/2025/05/11/Research/ReasonFlux%EF%BC%9A%20Hierarchical%20LLM%20Reasoning%20via%20Scaling%20Thought%20Templates/">ReasonFlux阅读</a></p><p class="categories"><a href="/categories/Research/">Research</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/09/19/NLP/%E6%89%8B%E6%92%95transformer/"><img src="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/avatar.jpg" alt="手撕transformer"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-09-19T13:14:00.000Z">2024-09-19</time></p><p class="title"><a href="/2024/09/19/NLP/%E6%89%8B%E6%92%95transformer/">手撕transformer</a></p><p class="categories"><a href="/categories/NLP/">NLP</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Research/"><span class="level-start"><span class="level-item">Research</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2025/05/"><span class="level-start"><span class="level-item">五月 2025</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/09/"><span class="level-start"><span class="level-item">九月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/07/"><span class="level-start"><span class="level-item">七月 2024</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95/"><span class="tag">数据结构算法</span><span class="tag is-grey-lightest">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"><span class="tag">编程语言</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ReasoningModel/"><span class="tag">ReasoningModel</span><span class="tag is-grey-lightest">3</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://cdn.jsdelivr.net/gh/1-pluto1/blog_imgs/b_0b65080deefa163507d3aa4c6a7a5e07.jpg" alt="Pluto&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2025 Pluto</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><div class="size-small"><span>❤️感谢 <strong><span id="busuanzi_value_site_uv">99+</span></strong> 小伙伴的 <strong><span id="busuanzi_value_site_pv">99+</span></strong> 次光临！❤️</span></div></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/1-pluto1"><i class="fab fa-github"></i></a></p></div><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="7589308811" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('21qqhhR0tOxfL1WbwUcK92DR-gzGzoHsz','e6qxlPdhLm33xH3N9UZP8Oay','akiya','undefined',true);})</script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('21qqhhR0tOxfL1WbwUcK92DR-gzGzoHsz','e6qxlPdhLm33xH3N9UZP8Oay','akiya','undefined',true);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script></body></html>